{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-2\n",
    "\n",
    "1. Implement Backpropagation algorithm to train an ANN of configuration __2x2x1__ to achieve __XOR__ function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: Neural network is defined and while training backpropagation algorithm is used for adjusting the weights so that the weights are adjusted making the neural network best fit to data as possible.\n",
    "\n",
    "The XOR truth table looks like:\n",
    "\n",
    "| Input      | Output |\n",
    "| ---------- | ------ |\n",
    "| 0    ,   0 |   0    |\n",
    "| 1    ,   0 |   1    |\n",
    "| 0    ,   1 |   1    |\n",
    "| 1    ,   1 |   0    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid( net_output ):\n",
    "        output = 1 / ( 1 + np.exp(-net_output))\n",
    "        return output\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, neurons, no_of_wghts, layer_no):\n",
    "        print(f\"Layer {layer_no+1} has {neurons} neurons and each neuron has {no_of_wghts} weights connected to it.\")\n",
    "        #create weights array of size (neurons, weights+1)\n",
    "        self.wght = np.random.normal(size=(neurons, no_of_wghts+1))\n",
    "        self.layer_no = layer_no+1\n",
    "        self.grad = np.empty(shape=(neurons, no_of_wghts+1))\n",
    "        self.delta = np.empty(shape=(1, neurons))\n",
    "    def activate_h(self, input):\n",
    "        #adding a single row for bias \n",
    "        _, col = input.shape\n",
    "        # print(input)\n",
    "        if self.wght.shape[1] == col:\n",
    "            self.activation = np.dot(input, self.wght.T)\n",
    "            # print(self.activation)\n",
    "            self.n_output = sigmoid(self.activation)\n",
    "            self.n_output = np.c_[self.n_output, np.random.normal(size=(self.n_output.shape[0], 1))]\n",
    "            return self.n_output\n",
    "        else:\n",
    "            raise Exception(f\"input {input.shape} and weights {self.wght.shape} shape not valid \")\n",
    "        \n",
    "    def activate_o(self, input):\n",
    "        #adding a single row for bias \n",
    "        _, col = input.shape\n",
    "        # print(input)\n",
    "        if self.wght.shape[1] == col:\n",
    "            self.activation = np.dot(input,self.wght.T )\n",
    "            # print(self.activation)\n",
    "            self.n_output = sigmoid(self.activation)\n",
    "            return self.n_output\n",
    "        else:\n",
    "            raise Exception(f\"input {input.shape} and weights {self.wght.shape} shape not valid \")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    def __init__(self, neurons, layer_no):\n",
    "        self.neurons = neurons\n",
    "        self.layer_no = layer_no+1\n",
    "    def activate_h(self, input):\n",
    "        self.n_output = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network:\n",
    "    def __init__(self, *args) -> None:\n",
    "        self.layers = []\n",
    "        self.total_layers = len(args)\n",
    "        # print(self.total_layers)\n",
    "        self.layers.append(Input(args[0],0))\n",
    "        for l in range(1, self.total_layers):\n",
    "            self.layers.append(Layer(args[l], args[l-1], l))\n",
    "        # print(self.layers)\n",
    "    def forward(self, input):\n",
    "        input = np.c_[input, np.random.normal(size=(input.shape[0],1))]\n",
    "        self.input = input\n",
    "        self.layers[0].activate_h(input)\n",
    "        for layer_no in range(1, self.total_layers-1):\n",
    "            input = self.layers[layer_no].activate_h(input)\n",
    "            # print(input, self.layers[layer_no].layer_no)\n",
    "        self.output = self.layers[-1].activate_o(input)\n",
    "        print(self.output, self.layers[-1].layer_no)\n",
    "    def backward(self, target, learning_rate=1):\n",
    "        diff = target - self.output\n",
    "        derivative = self.output * ( 1 - self.output)\n",
    "        delta = diff * derivative\n",
    "        # print(\"output=\")\n",
    "        # print(delta.T  @ self.layers[-2].n_output )\n",
    "        \n",
    "        # print(\"weight=\")\n",
    "        # print(self.layers[-1].wght)\n",
    "        # print(len(delta[:])) \n",
    "        #output layer weight update\n",
    "        for d in range(len(delta[:])):\n",
    "            # print(\"delta=\")\n",
    "            # print(delta[d])\n",
    "            print(\"grad_0=\")\n",
    "            grad  = np.dot(delta[d].T[:,np.newaxis], self.layers[-2].n_output[d][np.newaxis,:])\n",
    "            print(grad)\n",
    "            self.layers[-1].grad = grad\n",
    "            self.layers[-1].delta = delta[d]\n",
    "            # print(self.layers[-2].n_output[d][np.newaxis, :] )\n",
    "            #finding the gradients for weight update\n",
    "            for l in range(self.total_layers-1, 1, -1):\n",
    "                print(\"weights\")\n",
    "                print(self.layers[l].wght[:, :-1])\n",
    "                delta_h = np.dot(self.layers[l].delta, self.layers[l].wght[:, :-1]) # don't need the bias for the previous layers' weight update\n",
    "                deriv_h = (self.layers[l-1].n_output[d] * ( 1- self.layers[l-1].n_output[d]))[np.newaxis, :-1]\n",
    "                delta_h = delta_h * deriv_h\n",
    "                # print(delta_h.T, self.layers[l-2].n_output[d][np.newaxis, :]) \n",
    "                grad = np.dot(delta_h.T, self.layers[l-2].n_output[d][np.newaxis, :]) \n",
    "                self.layers[l-1].grad = grad\n",
    "                self.layers[l-1].delta = delta_h\n",
    "                # print(self.layers[l].layer_no)\n",
    "                # print(\"delta_h=\")\n",
    "                # print(delta_h)\n",
    "                # print(\"grad_h\")\n",
    "                # print(grad)\n",
    "                # print(\"hidden=\", self.layers[l-1].layer_no)\n",
    "                # print(self.layers[l-1].n_output[d] * ( 1- self.layers[l-1].n_output[d]))\n",
    "                # print(self.layers[l].delta.shape,  (self.layers[l-1].n_output[d] * ( 1- self.layers[l-1].n_output[d]))[np.newaxis, :].shape)\n",
    "                # grad  = np.dot(self.layers[].T[:,np.newaxis], self.layers[self.total_layers-(l+1)].n_output[d][np.newaxis,:])\n",
    "            \n",
    "            # print(\"grad=\")\n",
    "            # print(grad)\n",
    "        #     update = learning_rate * grad\n",
    "        #     # print(\"update=\", d)\n",
    "        #     # print(update)\n",
    "        #     #iterate through each examples' update array\n",
    "        #     for u in update:\n",
    "        #         self.layers[-1].wght[d] += u\n",
    "        #     print(\"weightchanged=\")\n",
    "        #     print(self.layers[-1].wght)\n",
    "                \n",
    "            #weight update\n",
    "            for layer in  range(1, self.total_layers):\n",
    "                pass\n",
    "                              \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 has 2 neurons and each neuron has 2 weights connected to it.\n",
      "Layer 3 has 1 neurons and each neuron has 2 weights connected to it.\n",
      "[[0.69861062]\n",
      " [0.90203384]] 3\n",
      "grad_0=\n",
      "[[-0.04482393 -0.01980102 -0.1394894 ]]\n",
      "weights\n",
      "[[0.57638394 1.86301057]]\n",
      "grad_0=\n",
      "[[-0.05574229 -0.06806963 -0.04124704]]\n",
      "weights\n",
      "[[0.57638394 1.86301057]]\n"
     ]
    }
   ],
   "source": [
    "net = Network(2, 2, 1)\n",
    "# test = Layer(2, 2, 1)\n",
    "input = np.array([[1, 0],\n",
    "                  [0, 1]])\n",
    "# print(input)\n",
    "# input = np.array([[1, 0]])\n",
    "net.forward(input)\n",
    "net.backward(np.array([[0],\n",
    "                      [0]]))\n",
    "# net.backward(np.array([[0, 0, 1, 1]]))\n",
    "# net.forward(input)\n",
    "# print(test.layers_wght)\n",
    "# act = test.wghted_sum(input)\n",
    "# print(act)\n",
    "# print(sigmoid(act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
