{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-2\n",
    "\n",
    "1. Implement Backpropagation algorithm to train an ANN of configuration __2x2x1__ to achieve __XOR__ function.\n",
    "2. Implement Backpropagation algorithm to train an ANN of configuration __3x2x2x1__ to achieve __majority function__ with 3-bit data. Output of the network must be 1 when there are two or more 1â€™s in the data.\n",
    "\n",
    "Neural network is defined and while training backpropagation algorithm is used for adjusting the weights so that the weights are adjusted making the neural network best fit to data as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "def sigmoid( net_output: npt.ArrayLike ) -> np.ndarray:\n",
    "        '''\n",
    "            Sigmoid activation function. \n",
    "            Takes an numpy array and computes value using the sigmoid function.\n",
    "        '''\n",
    "        output = 1 / ( 1 + np.exp(-net_output))\n",
    "        return output\n",
    "    \n",
    "class Layer:\n",
    "    '''\n",
    "        Class that defines each layer of neural network.\n",
    "        Each layer has a _n_ neurons and some weights connected to it.\n",
    "        ...\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        wght : ndarray\n",
    "            weights associated with each neurons.\n",
    "        layer_no : int\n",
    "            layer number in the neural network.\n",
    "        grad : ndarray\n",
    "            gradients for the weight update.\n",
    "        delta : ndarray\n",
    "            error term for the current layer.\n",
    "        activation: ndarray\n",
    "            weighted sum of each neuron.\n",
    "        n_output : ndarray\n",
    "            output of each neuron.\n",
    "        \n",
    "        \n",
    "        Methods\n",
    "        -------\n",
    "        activate_h(input):\n",
    "            Computes the weighted sum and outputs the activation value after \n",
    "            computing the sigmoid value\n",
    "            for each hidden neuron.\n",
    "        activate_o(input):\n",
    "            Computes the weighted sum and outputs the activation value after \n",
    "            computing the sigmoid value\n",
    "            for each outermost neuron.\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, neurons, no_of_wghts, layer_no):\n",
    "        print(f\"Layer {layer_no+1} has {neurons} neurons and each neuron has {no_of_wghts} weights connected to it.\")\n",
    "        self.wght = np.random.normal(size=(neurons, no_of_wghts+1))\n",
    "        self.layer_no = layer_no+1\n",
    "        self.grad = np.empty(shape=(neurons, no_of_wghts+1))\n",
    "        self.delta = np.empty(shape=(1, neurons))\n",
    "        \n",
    "    def activate_h(self, input):\n",
    "        '''\n",
    "            Computes the weighted sum and outputs the activation value after computing the sigmoid value for hidden neuron.\n",
    "        '''\n",
    "        _, col = input.shape\n",
    "        if self.wght.shape[1] == col:\n",
    "            self.activation = np.dot(input, self.wght.T)\n",
    "            self.n_output = sigmoid(self.activation)\n",
    "            self.n_output = np.c_[self.n_output, np.ones(shape=(self.n_output.shape[0], 1))]\n",
    "            return self.n_output\n",
    "        else:\n",
    "            raise Exception(f\"input {input.shape} and weights {self.wght.shape} shape not valid \")\n",
    "        \n",
    "    def activate_o(self, input):\n",
    "        '''\n",
    "            Computes the weighted sum and outputs the activation value after computing the sigmoid value for output layer. \n",
    "        '''\n",
    "        _, col = input.shape\n",
    "        if self.wght.shape[1] == col:\n",
    "            self.activation = np.dot(input,self.wght.T )\n",
    "            self.n_output = sigmoid(self.activation)\n",
    "            return self.n_output\n",
    "        else:\n",
    "            raise Exception(f\"input {input.shape} and weights {self.wght.shape} shape not valid \")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    def __init__(self, neurons, layer_no):\n",
    "        print(f\"Input Layer has {neurons} inputs.\")\n",
    "        self.neurons = neurons\n",
    "        self.layer_no = layer_no+1\n",
    "    def activate_h(self, input):\n",
    "        self.n_output = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network:\n",
    "    '''\n",
    "        Class defines and initializes the neural network.\n",
    "        ...\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        layers: list\n",
    "            list of all layers in the neural network.\n",
    "        total_layers: int\n",
    "            number of total layers in the network.\n",
    "        input : ndarray\n",
    "            input array for the neural network.\n",
    "        output : ndarray\n",
    "            output array for the neural network.\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        forward(input): None\n",
    "            computes the forward propagation phase of the neural network.\n",
    "        backward(target, learning_rate=1): None\n",
    "            computes the backward propagation phase of the neural network.\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, *args) -> None:\n",
    "        self.layers = []\n",
    "        self.total_layers = len(args)\n",
    "        self.layers.append(Input(args[0],0))\n",
    "        for l in range(1, self.total_layers):\n",
    "            self.layers.append(Layer(args[l], args[l-1], l))\n",
    "            \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        input = np.c_[input, np.ones(shape=(input.shape[0],1))]\n",
    "        self.layers[0].activate_h(input)\n",
    "        for layer_no in range(1, self.total_layers-1):\n",
    "            input = self.layers[layer_no].activate_h(input)\n",
    "        self.output = self.layers[-1].activate_o(input)\n",
    "        \n",
    "    def backward(self, target, learning_rate=1):\n",
    "        diff = target - self.output\n",
    "        derivative = self.output * ( 1 - self.output)\n",
    "        delta = diff * derivative\n",
    "        #output layer weight update\n",
    "        for d in range(len(self.output)):\n",
    "            diff = target[d] - self.output[d]\n",
    "            derivative = self.output[d] * (1 - self.output[d])\n",
    "            delta = diff * derivative\n",
    "            grad  = np.dot(delta.T[:,np.newaxis], self.layers[-2].n_output[d][np.newaxis,:])\n",
    "            self.layers[-1].grad = grad\n",
    "            self.layers[-1].delta = delta\n",
    "            #finding the gradients for weight update\n",
    "            for l in range(self.total_layers-1, 1, -1):\n",
    "                delta_h = np.dot(self.layers[l].delta, self.layers[l].wght[:, :-1]) # don't need the bias for the previous layers' weight update\n",
    "                deriv_h = (self.layers[l-1].n_output[d] * ( 1- self.layers[l-1].n_output[d]))[np.newaxis, :-1]\n",
    "                delta_h = delta_h * deriv_h\n",
    "                grad_h = np.dot(delta_h.T, self.layers[l-2].n_output[d][np.newaxis, :]) \n",
    "                self.layers[l-1].grad = grad_h\n",
    "                self.layers[l-1].delta = delta_h\n",
    "            #weight update\n",
    "            for layer in  range(1, self.total_layers):\n",
    "                self.layers[layer].wght = self.layers[layer].wght + (learning_rate * self.layers[layer].grad)\n",
    "            self.forward(self.input)     \n",
    "            \n",
    "    def train(self, input, output, learning_rate=1, epochs=1000):\n",
    "        for e in range(1, epochs+1):\n",
    "            self.forward(input)\n",
    "            self.backward(output, learning_rate=learning_rate)             \n",
    "            print(f\"{e} epochs completed.\", end=\"\\r\")\n",
    "        print(\"\",end='\\n')\n",
    "    \n",
    "    def test(self, input):\n",
    "        self.forward(input)\n",
    "        print(self.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Answer__ for __Q. No. 1__: \n",
    "\n",
    "The XOR truth table looks like:\n",
    "\n",
    "| Input      | Output |\n",
    "| ---------- | ------ |\n",
    "| 0    ,   0 |   0    |\n",
    "| 1    ,   0 |   1    |\n",
    "| 0    ,   1 |   1    |\n",
    "| 1    ,   1 |   0    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer has 2 inputs.\n",
      "Layer 2 has 2 neurons and each neuron has 2 weights connected to it.\n",
      "Layer 3 has 1 neurons and each neuron has 2 weights connected to it.\n",
      "1000 epochs completed.\n",
      "[[0.95257338]\n",
      " [0.94988354]\n",
      " [0.0559636 ]\n",
      " [0.0502493 ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "net = Network(2, 2, 1)\n",
    "\n",
    "# test = Layer(2, 2, 1)\n",
    "input = np.array([[1, 0],\n",
    "                  [0, 1],\n",
    "                  [0, 0],\n",
    "                  [1, 1]])\n",
    "output = np.array([ [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [0] ])\n",
    "net.train(input, output)\n",
    "net.test(input)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Answer__ for __Q. No. 2__: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer has 3 inputs.\n",
      "Layer 2 has 2 neurons and each neuron has 3 weights connected to it.\n",
      "Layer 3 has 2 neurons and each neuron has 2 weights connected to it.\n",
      "Layer 4 has 1 neurons and each neuron has 2 weights connected to it.\n",
      "1000 epochs completed.\n",
      "[[0.00968035]\n",
      " [0.02182572]\n",
      " [0.02065927]\n",
      " [0.97589547]\n",
      " [0.02059313]\n",
      " [0.97590901]\n",
      " [0.97536999]\n",
      " [0.98375435]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "net = Network(3, 2, 2, 1)\n",
    "arr = np.arange(0, 8)\n",
    "max_len = len(np.binary_repr(arr[-1]))\n",
    "input = np.array([list(np.binary_repr(x).zfill(max_len)) for x in arr], dtype=int)\n",
    "output = np.array([[0],\n",
    "                   [0],\n",
    "                   [0],\n",
    "                   [1],\n",
    "                   [0],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [1]])\n",
    "net.train(input, output)\n",
    "net.test(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
