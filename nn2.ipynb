{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-2\n",
    "\n",
    "1. Implement Backpropagation algorithm to train an ANN of configuration __2x2x1__ to achieve __XOR__ function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: Neural network is defined and while training backpropagation algorithm is used for adjusting the weights so that the weights are adjusted making the neural network best fit to data as possible.\n",
    "\n",
    "The XOR truth table looks like:\n",
    "\n",
    "| Input      | Output |\n",
    "| ---------- | ------ |\n",
    "| 0    ,   0 |   0    |\n",
    "| 1    ,   0 |   1    |\n",
    "| 0    ,   1 |   1    |\n",
    "| 1    ,   1 |   0    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid( net_output ):\n",
    "        output = 1 / ( 1 + np.exp(-net_output))\n",
    "        return output\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, neurons, no_of_wghts, layer_no):\n",
    "        print(f\"Layer {layer_no+1} has {neurons} neurons and each neuron has {no_of_wghts} weights connected to it.\")\n",
    "        #create weights array of size (neurons, weights+1)\n",
    "        self.wght = np.random.normal(size=(neurons, no_of_wghts+1))\n",
    "        self.layer_no = layer_no+1\n",
    "        self.grad = np.empty(shape=(neurons, no_of_wghts+1))\n",
    "        self.delta = np.empty(shape=(1, neurons))\n",
    "    def activate_h(self, input):\n",
    "        #adding a single row for bias \n",
    "        _, col = input.shape\n",
    "        # print(input)\n",
    "        if self.wght.shape[1] == col:\n",
    "            self.activation = np.dot(input, self.wght.T)\n",
    "            # print(self.activation)\n",
    "            self.n_output = sigmoid(self.activation)\n",
    "            self.n_output = np.c_[self.n_output, np.random.normal(size=(self.n_output.shape[0], 1))]\n",
    "            return self.n_output\n",
    "        else:\n",
    "            raise Exception(f\"input {input.shape} and weights {self.wght.shape} shape not valid \")\n",
    "        \n",
    "    def activate_o(self, input):\n",
    "        #adding a single row for bias \n",
    "        _, col = input.shape\n",
    "        # print(input)\n",
    "        if self.wght.shape[1] == col:\n",
    "            self.activation = np.dot(input,self.wght.T )\n",
    "            # print(self.activation)\n",
    "            self.n_output = sigmoid(self.activation)\n",
    "            return self.n_output\n",
    "        else:\n",
    "            raise Exception(f\"input {input.shape} and weights {self.wght.shape} shape not valid \")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    def __init__(self, neurons, layer_no):\n",
    "        self.neurons = neurons\n",
    "        self.layer_no = layer_no+1\n",
    "    def activate_h(self, input):\n",
    "        self.n_output = input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network:\n",
    "    def __init__(self, *args) -> None:\n",
    "        self.layers = []\n",
    "        self.total_layers = len(args)\n",
    "        # print(self.total_layers)\n",
    "        self.layers.append(Input(args[0],0))\n",
    "        for l in range(1, self.total_layers):\n",
    "            self.layers.append(Layer(args[l], args[l-1], l))\n",
    "        # print(self.layers)\n",
    "    def forward(self, input):\n",
    "        input = np.c_[input, np.random.normal(size=(input.shape[0],1))]\n",
    "        self.input = input\n",
    "        self.layers[0].activate_h(input)\n",
    "        for layer_no in range(1, self.total_layers-1):\n",
    "            input = self.layers[layer_no].activate_h(input)\n",
    "            # print(input, self.layers[layer_no].layer_no)\n",
    "        self.output = self.layers[-1].activate_o(input)\n",
    "        print(self.output, self.layers[-1].layer_no)\n",
    "    def backward(self, target, learning_rate=1):\n",
    "        diff = target - self.output\n",
    "        derivative = self.output * ( 1 - self.output)\n",
    "        delta = diff * derivative\n",
    "        # print(\"output=\")\n",
    "        # print(delta.T  @ self.layers[-2].n_output )\n",
    "        \n",
    "        # print(\"weight=\")\n",
    "        # print(self.layers[-1].wght)\n",
    "        # print(len(delta[:])) \n",
    "        #output layer weight update\n",
    "        for d in range(len(delta[:])):\n",
    "            # print(\"delta=\")\n",
    "            # print(delta[d])\n",
    "            print(\"grad_0=\")\n",
    "            grad  = np.dot(delta[d].T[:,np.newaxis], self.layers[-2].n_output[d][np.newaxis,:])\n",
    "            print(grad)\n",
    "            self.layers[-1].grad = grad\n",
    "            self.layers[-1].delta = delta[d]\n",
    "            # print(self.layers[-2].n_output[d][np.newaxis, :] )\n",
    "            #finding the gradients for weight update\n",
    "            for l in range(self.total_layers-1, 1, -1):\n",
    "                print(\"weights\")\n",
    "                print(self.layers[l].wght[:, :-1])\n",
    "                delta_h = np.dot(self.layers[l].delta, self.layers[l].wght[:, :-1]) # don't need the bias for the previous layers' weight update\n",
    "                deriv_h = (self.layers[l-1].n_output[d] * ( 1- self.layers[l-1].n_output[d]))[np.newaxis, :-1]\n",
    "                delta_h = delta_h * deriv_h\n",
    "                # print(delta_h.T, self.layers[l-2].n_output[d][np.newaxis, :]) \n",
    "                grad = np.dot(delta_h.T, self.layers[l-2].n_output[d][np.newaxis, :]) \n",
    "                self.layers[l-1].grad = grad\n",
    "                self.layers[l-1].delta = delta_h\n",
    "                # print(self.layers[l].layer_no)\n",
    "                # print(\"delta_h=\")\n",
    "                # print(delta_h)\n",
    "                # print(\"grad_h\")\n",
    "                # print(grad)\n",
    "                # print(\"hidden=\", self.layers[l-1].layer_no)\n",
    "                # print(self.layers[l-1].n_output[d] * ( 1- self.layers[l-1].n_output[d]))\n",
    "                # print(self.layers[l].delta.shape,  (self.layers[l-1].n_output[d] * ( 1- self.layers[l-1].n_output[d]))[np.newaxis, :].shape)\n",
    "                # grad  = np.dot(self.layers[].T[:,np.newaxis], self.layers[self.total_layers-(l+1)].n_output[d][np.newaxis,:])\n",
    "            \n",
    "            # print(\"grad=\")\n",
    "            # print(grad)\n",
    "        #     update = learning_rate * grad\n",
    "        #     # print(\"update=\", d)\n",
    "        #     # print(update)\n",
    "        #     #iterate through each examples' update array\n",
    "        #     for u in update:\n",
    "        #         self.layers[-1].wght[d] += u\n",
    "        #     print(\"weightchanged=\")\n",
    "        #     print(self.layers[-1].wght)\n",
    "                \n",
    "            #weight update\n",
    "            for layer in  range(1, self.total_layers):\n",
    "                pass\n",
    "                              \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 has 2 neurons and each neuron has 2 weights connected to it.\n",
      "Layer 3 has 1 neurons and each neuron has 2 weights connected to it.\n",
      "[[0.30358451]\n",
      " [0.3635475 ]] 3\n",
      "grad_0=\n",
      "[[-0.03851097 -0.02633677 -0.02419559]]\n",
      "weights\n",
      "[[-0.41726119 -1.09589139]]\n",
      "grad_0=\n",
      "[[-0.00651003 -0.02431251 -0.0513609 ]]\n",
      "weights\n",
      "[[-0.41726119 -1.09589139]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Input' object has no attribute 'wght'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[349], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# print(input)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# input = np.array([[1, 0]])\u001b[39;00m\n\u001b[0;32m      7\u001b[0m net\u001b[39m.\u001b[39mforward(\u001b[39minput\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m net\u001b[39m.\u001b[39;49mbackward(np\u001b[39m.\u001b[39;49marray([[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m      9\u001b[0m                       [\u001b[39m0\u001b[39;49m]]))\n\u001b[0;32m     10\u001b[0m \u001b[39m# net.backward(np.array([[0, 0, 1, 1]]))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# net.forward(input)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# print(test.layers_wght)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# act = test.wghted_sum(input)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# print(act)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# print(sigmoid(act))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[348], line 73\u001b[0m, in \u001b[0;36mNetwork.backward\u001b[1;34m(self, target, learning_rate)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[39m# print(self.layers[l].layer_no)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m         \u001b[39m# print(\"delta_h=\")\u001b[39;00m\n\u001b[0;32m     52\u001b[0m         \u001b[39m# print(delta_h)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \n\u001b[0;32m     71\u001b[0m \u001b[39m#weight update\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 73\u001b[0m     layer\u001b[39m.\u001b[39mwght  \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mwght \u001b[39m+\u001b[39m (learning_rate \u001b[39m*\u001b[39m layer\u001b[39m.\u001b[39mgrad)\n\u001b[0;32m     74\u001b[0m     \u001b[39mprint\u001b[39m(layer\u001b[39m.\u001b[39mwght, layer\u001b[39m.\u001b[39mlayer_no)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Input' object has no attribute 'wght'"
     ]
    }
   ],
   "source": [
    "net = Network(2, 2, 1)\n",
    "# test = Layer(2, 2, 1)\n",
    "input = np.array([[1, 0],\n",
    "                  [0, 1]])\n",
    "# print(input)\n",
    "# input = np.array([[1, 0]])\n",
    "net.forward(input)\n",
    "net.backward(np.array([[0],\n",
    "                      [0]]))\n",
    "# net.backward(np.array([[0, 0, 1, 1]]))\n",
    "# net.forward(input)\n",
    "# print(test.layers_wght)\n",
    "# act = test.wghted_sum(input)\n",
    "# print(act)\n",
    "# print(sigmoid(act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
